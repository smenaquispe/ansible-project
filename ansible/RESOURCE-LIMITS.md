# ConfiguraciÃ³n de LÃ­mites de CPU y Memoria

## âœ… Cambios Realizados

Ahora puedes configurar **lÃ­mites mÃ­nimos y mÃ¡ximos de CPU y memoria** directamente en `group_vars/all.yml`.

## ðŸ“ Variables en `all.yml`

```yaml
# LÃ­mites de Recursos para Autoescalamiento
resource_limits:
  cpu:
    min: 4 # MÃ­nimo de vCPUs en el cluster
    max: 10 # MÃ¡ximo de vCPUs en el cluster
  memory:
    min: 4 # MÃ­nimo de GB de memoria en el cluster
    max: 10 # MÃ¡ximo de GB de memoria en el cluster
```

## ðŸŽ¯ CÃ³mo Funciona

GKE escalarÃ¡ el nÃºmero de nodos para mantener el cluster dentro de estos lÃ­mites de recursos:

### Ejemplo con `e2-small` (2 vCPUs, 2 GB RAM por nodo)

```yaml
resource_limits:
  cpu:
    min: 4 # Al menos 2 nodos (2 nodos Ã— 2 vCPUs = 4 vCPUs)
    max: 10 # MÃ¡ximo 5 nodos (5 nodos Ã— 2 vCPUs = 10 vCPUs)
  memory:
    min: 4 # Al menos 2 nodos (2 nodos Ã— 2 GB = 4 GB)
    max: 10 # MÃ¡ximo 5 nodos (5 nodos Ã— 2 GB = 10 GB)
```

**Resultado:** El cluster tendrÃ¡ entre 2 y 5 nodos.

### Ejemplo con LÃ­mites No Alineados

```yaml
resource_limits:
  cpu:
    min: 6 # Al menos 3 nodos (3 nodos Ã— 2 vCPUs = 6 vCPUs)
    max: 12 # MÃ¡ximo 6 nodos (6 nodos Ã— 2 vCPUs = 12 vCPUs)
  memory:
    min: 4 # Al menos 2 nodos (2 nodos Ã— 2 GB = 4 GB)
    max: 16 # MÃ¡ximo 8 nodos (8 nodos Ã— 2 GB = 16 GB)
```

**Resultado:**

- Por CPU: Necesita 3-6 nodos
- Por memoria: Necesita 2-8 nodos
- GKE usarÃ¡ el lÃ­mite mÃ¡s restrictivo: **3-6 nodos**

## ðŸ“Š Ejemplos PrÃ¡cticos

### 1. Cluster PequeÃ±o (Desarrollo)

```yaml
machine_type: e2-small # 2 vCPUs, 2 GB RAM

resource_limits:
  cpu:
    min: 2 # 1 nodo mÃ­nimo
    max: 6 # 3 nodos mÃ¡ximo
  memory:
    min: 2 # 1 nodo mÃ­nimo
    max: 6 # 3 nodos mÃ¡ximo

min_nodes: 1
max_nodes: 3
```

### 2. Cluster Mediano (ProducciÃ³n)

```yaml
machine_type: e2-medium # 2 vCPUs, 4 GB RAM

resource_limits:
  cpu:
    min: 4 # 2 nodos mÃ­nimo
    max: 20 # 10 nodos mÃ¡ximo
  memory:
    min: 8 # 2 nodos mÃ­nimo
    max: 40 # 10 nodos mÃ¡ximo

min_nodes: 2
max_nodes: 10
```

### 3. Cluster Grande (Alta Demanda)

```yaml
machine_type: e2-standard-4 # 4 vCPUs, 16 GB RAM

resource_limits:
  cpu:
    min: 12 # 3 nodos mÃ­nimo (3 Ã— 4 = 12 vCPUs)
    max: 60 # 15 nodos mÃ¡ximo (15 Ã— 4 = 60 vCPUs)
  memory:
    min: 48 # 3 nodos mÃ­nimo (3 Ã— 16 = 48 GB)
    max: 240 # 15 nodos mÃ¡ximo (15 Ã— 16 = 240 GB)

min_nodes: 3
max_nodes: 15
```

## ðŸ”§ CÃ³mo Calcular los LÃ­mites

### Paso 1: Conoce tu Tipo de MÃ¡quina

| Tipo de MÃ¡quina | vCPUs | Memoria (GB) |
| --------------- | ----- | ------------ |
| e2-micro        | 2     | 1            |
| e2-small        | 2     | 2            |
| e2-medium       | 2     | 4            |
| e2-standard-2   | 2     | 8            |
| e2-standard-4   | 4     | 16           |
| e2-standard-8   | 8     | 32           |

### Paso 2: Calcula Recursos Totales

```
CPU total = nÃºmero_de_nodos Ã— vCPUs_por_nodo
Memoria total = nÃºmero_de_nodos Ã— GB_por_nodo
```

### Paso 3: Define tus LÃ­mites

```yaml
resource_limits:
  cpu:
    min: min_nodes Ã— vCPUs_por_nodo
    max: max_nodes Ã— vCPUs_por_nodo
  memory:
    min: min_nodes Ã— GB_por_nodo
    max: max_nodes Ã— GB_por_nodo
```

## ðŸ’¡ Consejos

### 1. Alinea con min_nodes y max_nodes

Para evitar confusiÃ³n, asegÃºrate de que los lÃ­mites sean consistentes:

```yaml
# âœ… CORRECTO - Alineado
machine_type: e2-small # 2 vCPUs, 2 GB RAM
min_nodes: 2
max_nodes: 5

resource_limits:
  cpu:
    min: 4 # 2 nodos Ã— 2 vCPUs
    max: 10 # 5 nodos Ã— 2 vCPUs
  memory:
    min: 4 # 2 nodos Ã— 2 GB
    max: 10 # 5 nodos Ã— 2 GB
```

### 2. Considera tus Pods

Si tus pods requieren muchos recursos, aumenta los lÃ­mites:

```yaml
# Si cada pod requiere 500m CPU y 512Mi memoria
# Y quieres correr hasta 20 pods:
# Necesitas: 20 Ã— 0.5 = 10 vCPUs mÃ­nimo
# Y:        20 Ã— 0.5 = 10 GB memoria mÃ­nimo

resource_limits:
  cpu:
    min: 4 # Base
    max: 20 # Para 20 pods con headroom
  memory:
    min: 4 # Base
    max: 20 # Para 20 pods con headroom
```

### 3. Deja Margen para el Sistema

Kubernetes y GKE usan recursos del nodo:

- ~10-20% de CPU
- ~100-500 MB de memoria

```yaml
# âŒ MALO - Sin margen
resource_limits:
  cpu:
    max: 10  # Tus pods necesitan exactamente 10 vCPUs

# âœ… BUENO - Con margen
resource_limits:
  cpu:
    max: 12  # 10 para tus pods + 2 de margen
```

## ðŸš€ CÃ³mo Usar

### 1. Configurar en all.yml

```bash
# Edita el archivo
nano ansible/group_vars/all.yml
```

```yaml
# Configura tus lÃ­mites
resource_limits:
  cpu:
    min: 4
    max: 10
  memory:
    min: 4
    max: 10
```

### 2. Crear Cluster

```bash
ansible-playbook -i ansible/inventory/hosts ansible/playbooks/create-cluster.yml
```

### 3. Verificar LÃ­mites

```bash
# Ver configuraciÃ³n del cluster
gcloud container clusters describe todo-app-cluster \
  --zone=us-central1-a \
  --format="yaml(autoscaling, clusterAutoscaling)"
```

### 4. Actualizar LÃ­mites

```bash
# Edita all.yml con nuevos valores
# Luego:
ansible-playbook -i ansible/inventory/hosts ansible/playbooks/update-cluster.yml
```

### 5. Override en Runtime

```bash
ansible-playbook -i ansible/inventory/hosts ansible/playbooks/create-cluster.yml \
  -e '{"resource_limits": {"cpu": {"min": 8, "max": 20}, "memory": {"min": 8, "max": 20}}}'
```

## ðŸ“Š Monitoreo

### Ver Uso Actual

```bash
# Uso total del cluster
kubectl top nodes

# Suma de CPU y memoria
kubectl top nodes | awk 'NR>1 {cpu+=$3; mem+=$5} END {print "CPU:", cpu, "Memoria:", mem}'
```

### Ver Capacidad Total

```bash
# Ver capacidad de cada nodo
kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, cpu: .status.capacity.cpu, memory: .status.capacity.memory}'
```

## âš ï¸ Advertencias

1. **Los lÃ­mites deben ser realistas**: No pongas lÃ­mites muy bajos que impidan que tus pods se programen

2. **GKE no puede crear fracciones de nodo**: Si pones `cpu.max: 5` con `e2-small` (2 vCPUs), GKE crearÃ¡ mÃ¡ximo 2 nodos (4 vCPUs), no 2.5

3. **Los lÃ­mites son guÃ­as, no garantÃ­as**: GKE usa estos valores junto con otras mÃ©tricas para decidir cuÃ¡ndo escalar

## ðŸ“š RelaciÃ³n con Otras Variables

```yaml
# Estas variables trabajan juntas:

# 1. LÃ­mites fÃ­sicos de nodos
min_nodes: 2
max_nodes: 5

# 2. LÃ­mites de recursos totales
resource_limits:
  cpu:
    min: 4 # min_nodes Ã— vCPUs
    max: 10 # max_nodes Ã— vCPUs

# 3. Umbrales de utilizaciÃ³n
cluster_autoscaler_settings:
  cpu_utilization_target: 0.7 # Escalar cuando uso > 70%

# 4. Perfil de comportamiento
autoscaling_profile: balanced
```

## ðŸŽ‰ Resultado

Ahora puedes controlar el autoescalamiento tanto por **cantidad de nodos** como por **cantidad de recursos (CPU/memoria)**.

Ejemplo final en `all.yml`:

```yaml
# ConfiguraciÃ³n completa
machine_type: e2-small # 2 vCPUs, 2 GB RAM

min_nodes: 2
max_nodes: 5

resource_limits:
  cpu:
    min: 4 # 2 nodos Ã— 2 vCPUs
    max: 10 # 5 nodos Ã— 2 vCPUs
  memory:
    min: 4 # 2 nodos Ã— 2 GB
    max: 10 # 5 nodos Ã— 2 GB

cluster_autoscaler_settings:
  cpu_utilization_target: 0.7
  memory_utilization_target: 0.8
```

Â¡Ahora tienes control completo! ðŸš€
